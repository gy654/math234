# -*- coding: utf-8 -*-
"""jackknife_share.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1biJwS6-D5191Q3rxtpSTYl-FVy0oMIDo

In this notebook we implement the jackknife for a correlation estimation problem.

Let $W,Z \sim N(0,1)$ be independent. Define $X = W + \gamma Z$ and $Y = Z + \gamma W$.

Then
$$
\mathbb{E}[X] = \mathbb{E}[Y] = 0, \quad \mathbb{E}[X^2] = \mathbb{E}[Y^2] = 1 + \gamma^2, \quad \mathbb{E}[XY] = 2\gamma, 
$$ 
so
$$
\rho_{X,Y} = \frac{\mathbb{E}[XY]}{\sqrt{ \mathbb{E}[X^2]\, \mathbb{E}[Y^2] }}  = \frac{2 \gamma }{1+\gamma^2}.
$$
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erfinv

"""We will run many experiments, each with many samples. First we choose parameters: $\gamma$, number $n$ of samples per experiment, number $N_e$ of experiments, and confidence interval parameter $\alpha$."""

gamma = .1        # model parameter
alpha = .05   # confidence interval parameter
n = 100       # number of samples per experiment
Ne = 100000   # number of experiments

"""Then we generate data."""

W = np.random.randn(Ne,n)
Z = np.random.randn(Ne,n)
X = W + gamma*Z
Y = Z + gamma*W

"""Then we compute the exact correlation $\rho$ and the plug-in estimator $\hat{\rho}_n$ for the correlation."""

# exact correlation
rho_exact = (2*gamma)/(1+gamma**2)
# estimated correlation for each each experiment (vector of length Ne)
rho_hat = np.mean(X*Y,1) / np.sqrt( np.mean(X*X,1) * np.mean(Y*Y,1) )

"""We also compute all of the 'leave-one-out' ('loo') versions $\hat{\rho}_{(-i)}$ of the covariance estimator."""

# rho_loo[k,i] will correspond to k-th experiment, i-th sample left out

## A MORE STRAIGHTFORWARD APPROACH
# rho_loo = np.zeros((Ne,n))
# num_ref = np.sum(X*Y,1)
# denom1_ref = np.sum(X*X,1)
# denom2_ref = np.sum(Y*Y,1)
# for i in range(n):
#   numerator = (num_ref - X[:,i]*Y[:,i])/(n-1)
#   denom1 = (denom1_ref - X[:,i]*X[:,i])/(n-1)
#   denom2 = (denom2_ref - Y[:,i]*Y[:,i])/(n-1)
#   rho_loo[:,i] = numerator/np.sqrt(denom1*denom2)

## SLICK APPROACH (no loops)
num_ref = np.reshape(np.sum(X*Y,1),(Ne,1))
denom1_ref = np.reshape(np.sum(X*X,1),(Ne,1))
denom2_ref = np.reshape(np.sum(Y*Y,1),(Ne,1))
rho_loo = (num_ref - X*Y) / np.sqrt( (denom1_ref - X*X)*(denom2_ref-Y*Y) )

"""Then we compute $\overline{\rho}_n = \frac{1}{n} \sum_{i=1}^n \hat{\rho}_{(-i)}$."""

rho_bar = np.mean(rho_loo,1)

"""Next we compute the jackknife estimate of the covariance
$$
v_{\mathrm{jack}} = \frac{n-1}{n} \sum_{i=1}^n \left( \hat{\rho}_{(-i)} - \overline{\rho}_n \right)^2
$$
and the standard error $\widehat{\textsf{se}}_{\mathrm{jack}} = \sqrt{v_{\mathrm{jack}}}$. 
"""

v_jack = (n-1) * np.mean( ( rho_loo - np.reshape(rho_bar,(Ne,1)) )**2 , 1 )
se_jack = np.sqrt(v_jack)

"""Meanwhile we can compute empirical estimates for $\mathbb{V}(\hat{\rho}_n)$ and $\mathsf{se}(\hat{\rho}_n)$  by averaging over many experiments."""

v_emp = (1/(Ne-1)) * np.sum( (rho_hat - np.mean(rho_hat))**2)
se_emp = np.sqrt( v_emp )

"""Let us then plot a histogram of the relative error of the standard error estimator 
$$
\frac{\widehat{\textsf{se}}_{\mathrm{jack}} - \mathsf{se}(\hat{\rho}_n)}{ \mathsf{se}(\hat{\rho}_n)}.
$$
"""

plt.hist( (se_jack -  se_emp)/(se_emp), bins = int(Ne/100));
plt.xlabel("$ ( \widehat{\mathsf{se}}_{\mathrm{jack}} - \mathsf{se} ) \  / \   \mathsf{se} $", fontsize = 16);

"""Next we use the jackknife to estimate the bias. Recall
$$
\widehat{\mathsf{bias}}_{\mathrm{jack}} = (n-1) \left( \overline{\rho}_n - \hat{\rho}_n \right).
$$

We also compute the jackknife estimator 
$$
\hat{\rho}_{\mathrm{jack}} = \hat{\rho}_n - \widehat{\mathsf{bias}}_{\mathrm{jack}}.
$$
"""

bias_jack = (n-1)*(rho_bar - rho_hat)
rho_jack  = rho_hat - bias_jack

"""Let us then plot a histogram of the error $\hat{\rho}_{\mathrm{jack}} - \rho_{X,Y}$ of the jackknife estimator."""

plt.hist( rho_jack-rho_exact , bins = int(Ne/100));
plt.xlabel("Bias-corrected estimator error", fontsize = 16);

"""We compare this against the error histogram for the original estimator error $\hat{\rho}_n - \rho_{X,Y}$."""

plt.hist( rho_hat-rho_exact , bins = int(Ne/100));
plt.xlabel("Original estimator error", fontsize = 16);

"""To conclude we print the biases and root MSEs of the jackknife estimator and the original estimator, estimated empirically by averaging over many experiments."""

bias_emp = np.mean(rho_hat - rho_exact)
bias_emp_jack = np.mean(rho_jack - rho_exact)
RMSE = np.sqrt( np.mean( (rho_hat - rho_exact)**2 ) )
RMSE_jack = np.sqrt( np.mean( (rho_jack - rho_exact)**2 ) )

print("Estimated estimator bias: ", bias_emp)
print("Estimated jackknife estimator bias: ", bias_emp_jack)
print("Estimator RMSE: ", RMSE)
print("Jackknife estimator RMSE: ", RMSE_jack)

"""Finally we use the jackknife standard error to construct a normal confidence interval."""

z = erfinv(1-alpha/2)
a_normal = rho_hat - z*se_jack
b_normal = rho_hat + z*se_jack

"""We check empirically over many experiments the probability that the normal confidence interval succeeds, i.e., that the true value $\rho_{X,Y}$ lies in the confidence interval."""

P_normal = np.sum( (a_normal <= rho_exact)*(b_normal >= rho_exact) )/Ne
print("Probability of success: ", P_normal)

